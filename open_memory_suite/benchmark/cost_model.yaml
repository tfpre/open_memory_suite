# Open Memory Suite - Cost Model Configuration
# All costs in USD cents (0.0001 = $0.0001)
# Updated: August 6, 2025 - Current market pricing

# Storage Operations - Cost to store a single memory item
storage:
  # In-Memory Adapter (free, ephemeral)
  memory_store:
    store_item: 0.0000          # Free - just memory allocation
    maintain_index: 0.0000      # Free - TF-IDF computation is local
    
  # FAISS Vector Store (local compute)
  faiss_store:
    embedding_generation: 0.0001 # Local SentenceTransformer inference
    vector_store: 0.0000        # Free - local FAISS index
    index_maintenance: 0.0001   # Periodic index optimization
    persistence_save: 0.0001    # File I/O cost
    
  # File Store (cheapest persistence)
  file_store:
    file_write: 0.00001         # Ultra-cheap file append
    directory_scan: 0.00001     # Cost scales with file count
    json_serialization: 0.00001 # CPU cost for JSON encoding
    
  # Zep Graph Store (enterprise-grade, API-based)
  zep_store:
    entity_extraction: 0.05     # Complex NLP processing
    graph_write: 0.02           # Neo4j database operation
    relationship_inference: 0.03 # Graph analysis
    api_call_overhead: 0.01     # Network + auth costs

# Retrieval Operations - Cost to query and retrieve memories
retrieval:
  # In-Memory Adapter
  memory_store:
    tfidf_search: 0.0000        # Free - local computation
    similarity_ranking: 0.0000  # Free - cosine similarity
    
  # FAISS Vector Store  
  faiss_store:
    query_embedding: 0.0001     # SentenceTransformer inference
    vector_search: 0.0001       # FAISS similarity search
    result_ranking: 0.0000      # Free - already ranked by FAISS
    
  # File Store
  file_store:
    linear_scan: 0.0001         # Cost scales with storage size
    keyword_matching: 0.00001   # Simple string operations
    file_io: 0.00001           # Reading file contents
    
  # Zep Graph Store
  zep_store:
    semantic_query: 0.03        # Complex graph traversal
    context_assembly: 0.02      # Building response context
    api_call_overhead: 0.01     # Network costs

# Content Processing - Advanced operations
processing:
  # Summarization costs (current 2025 pricing)
  summarization:
    gpt_4o_mini: 0.08           # $0.15 input + $0.60 output per 1M tokens (~1000 tokens)
    gpt_4o: 0.25                # $2.50 input + $10.00 output per 1M tokens
    claude_3_haiku: 0.04        # $0.25 input + $1.25 output per 1M tokens
    claude_3_sonnet: 0.18       # $3.00 input + $15.00 output per 1M tokens
    local_llama_7b: 0.0001      # Local inference (amortized GPU cost)
    local_llama_13b: 0.0002     # Larger model, more compute
    
  # Content analysis 
  analysis:
    importance_scoring: 0.00001  # Local ML inference
    entity_detection: 0.0001    # NER model inference
    keyword_extraction: 0.00001 # Simple text processing
    sentiment_analysis: 0.00001 # Local model
    
  # Memory management
  maintenance:
    memory_compression: 0.0005  # LLM-powered compression
    duplicate_detection: 0.0001 # Similarity comparison
    archival_migration: 0.00005 # Moving old memories to cheaper storage
    index_rebuilding: 0.001     # Periodic optimization

# Adapter-specific multipliers (for scaling costs)
multipliers:
  # Scale factors based on content size
  content_size:
    small: 1.0      # <100 tokens
    medium: 1.5     # 100-500 tokens  
    large: 2.0      # 500-1000 tokens
    xlarge: 3.0     # >1000 tokens
    
  # Scale factors based on memory pressure
  memory_pressure:
    low: 1.0        # <1000 items stored
    medium: 1.2     # 1000-10000 items
    high: 1.5       # 10000-100000 items
    critical: 2.0   # >100000 items
    
  # Scale factors for concurrent operations
  concurrency:
    single: 1.0     # Single user
    light: 1.1      # <10 concurrent users
    moderate: 1.3   # 10-100 concurrent users  
    heavy: 1.8      # >100 concurrent users

# Latency targets (milliseconds) - for SLA compliance
latency_targets:
  storage:
    memory_store: 1      # Sub-millisecond
    faiss_store: 10      # Local embedding + store
    file_store: 50       # File I/O latency
    zep_store: 200       # Network + processing
    
  retrieval:
    memory_store: 1      # Instant TF-IDF
    faiss_store: 5       # Vector search is fast
    file_store: 100      # Linear scan penalty
    zep_store: 150       # Graph query complexity
    
  processing:
    summarization: 2000  # LLM inference time
    analysis: 50         # Local ML models
    maintenance: 1000    # Background operations

# Budget constraints (cents) - for dispatcher decision making
budgets:
  # Per-conversation cost limits
  conversation:
    minimal: 1.0         # Ultra-frugal mode
    standard: 5.0        # Balanced mode
    premium: 20.0        # High-quality mode
    unlimited: 1000.0    # Research/development mode
    
  # Per-operation limits
  operation:
    storage_max: 0.1     # Never spend more than 10 cents to store one item
    retrieval_max: 0.05  # Max 5 cents per query
    processing_max: 0.5  # Max 50 cents for summarization
    
  # Daily/session limits
  session:
    daily_limit: 50.0    # $0.50 per day per user
    session_limit: 10.0  # $0.10 per conversation session
    burst_limit: 2.0     # $0.02 for sudden spikes

# Quality thresholds - minimum acceptable performance
quality_thresholds:
  recall:
    minimum_acceptable: 0.80    # 80% recall floor
    target: 0.90               # 90% recall target
    excellent: 0.95            # 95% aspirational
    
  latency:
    interactive_max: 200       # 200ms for real-time chat
    batch_max: 5000           # 5s for background processing
    
  cost_efficiency:
    target_reduction: 0.70     # 70% cost reduction vs baseline (our ambitious target)
    minimum_reduction: 0.40    # 40% minimum to be worthwhile
    
# Real-World Cost Baselines (cents per 50-turn conversation)
# Used for demonstrating cost savings and ROI calculations  
cost_baselines:
  naive_store_everything:
    storage_cost: 12.0         # Store every turn in expensive vector DB
    retrieval_cost: 8.0        # Linear scan all stored items
    total_per_conversation: 20.0
    description: "Naive approach - no intelligence"
    
  basic_rag_system:
    storage_cost: 8.0          # Simple vector storage
    retrieval_cost: 4.0        # Basic vector similarity 
    total_per_conversation: 12.0
    improvement_vs_naive: 0.40  # 40% better than naive
    description: "Standard RAG with FAISS"
    
  intelligent_routing:
    storage_cost: 3.5          # Smart routing decisions
    retrieval_cost: 2.0        # Optimized multi-adapter retrieval
    total_per_conversation: 5.5
    improvement_vs_naive: 0.725 # 72.5% better than naive
    improvement_vs_basic: 0.54  # 54% better than basic RAG
    description: "Our target performance"
    
# XGBoost Training Cost Estimates
ml_training_costs:
  data_generation:
    synthetic_conversations: 15.0    # GPT-4 auto-labeling 14k examples
    real_world_labeling: 8.0        # Human validation on 2k examples  
    active_learning: 5.0            # Single round uncertainty sampling
    total_data_cost: 28.0           # One-time cost
    
  model_training:
    xgboost_training: 0.05          # Local compute cost
    distilbert_lora: 2.0            # GPU hours for fine-tuning
    evaluation_runs: 1.0            # Cross-validation and testing
    total_training_cost: 3.05       # One-time cost
    
  total_ml_investment: 31.05        # Total one-time cost
  break_even_conversations: 50     # Pays for itself after ~50 conversations